---
layout: post
title: "Understanding the Maths Behind Naive Bayes Model"
subtitle: "Delving into the mathematical foundations of algorithms and their practical applications by building code
implementations from scratch."
date: 2023-07-18 08:45:13 -0400
background: '/img/posts/NB_01.jpeg'
---

<style>
    body {
        background-image: url('https://thumbs.gfycat.com/UltimateMeekArmedcrab-size_restricted.gif');
        background-size: cover;
        height: 100vh;
        padding: 0;
        margin: 0;
    }

    p {
        text-align: justify;
    }

    .github-link {
        display: flex;
        align-items: left;
        justify-content: left;
        text-align: left;
        color: #0000FF;
        font-style: italic;
        font-size: medium;
        text-decoration: none;
    }

    .github-link img {
        width: 40px;
        height: 40px;
        margin-right: 5px;
    }

    #myBtn {
        display: none;
        position: fixed;
        bottom: 20px;
        right: 30px;
        z-index: 99;
        font-size: 15px;
        border: none;
        outline: none;
        background-color: rgb(238, 208, 37);
        color: white;
        cursor: pointer;
        padding: 10px;
        border-radius: 4px;
    }

    #myBtn:hover {
        background-color: #555;
    }
</style>

<button onclick="topFunction()" id="myBtn" title="Back to top">
    <img class="img-fluid" src="/img/posts/arrow.jpg" height="30" width="30">
</button>

<script>
    document.addEventListener('contextmenu', (e) => {
        e.preventDefault();
    }
    );
    document.onkeydown = function (e) {
        if (event.keyCode == 123) {
            return false;
        }
        if (e.ctrlKey && e.shiftKey && e.keyCode == 'I'.charCodeAt(0)) {
            return false;
        }
        if (e.ctrlKey && e.shiftKey && e.keyCode == 'C'.charCodeAt(0)) {
            return false;
        }
        if (e.ctrlKey && e.shiftKey && e.keyCode == 'J'.charCodeAt(0)) {
            return false;
        }
        if (e.ctrlKey && e.keyCode == 'U'.charCodeAt(0)) {
            return false;
        }
    }

    var mybutton = document.getElementById("myBtn");
    var message = "Right- Click Function Disabled!";

    // When the user scrolls down 20px from the top of the document, show the button
    window.onscroll = function () { scrollFunction() };

    function scrollFunction() {
        if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
            mybutton.style.display = "block";
        } else {
            mybutton.style.display = "none";
        }
    }

    // When the user clicks on the button, scroll to the top of the document
    function topFunction() {
        document.body.scrollTop = 0;
        document.documentElement.scrollTop = 0;
    }

    function disableselect(e) {
        return false
    }

    function reEnable() {
        return true
    }

    //if IE4+  
    document.onselectstart = new Function("return false")
    document.oncontextmenu = new Function("return false")
    document.oncontextmenu = new Function("alert(message);return false")

    //if NS6  
    if (window.sidebar) {
        document.onmousedown = disableselect
        document.onclick = reEnable

    }

</script>

<html>

<head>
    <title>Understanding Naive Bayes: A Step-by-Step Guide</title>
</head>

<body>
    <h1>Understanding Naive Bayes: A Step-by-Step Guide</h1>

    <p>In the captivating realm of machine learning, classification algorithms wield an enchanting power to predict the
        class label of data instances. Among the most enchanting ones is Naive Bayes, a spellbindingly simple yet
        surprisingly powerful algorithm. In this blog post, we embark on a magical journey into the mathematical
        underpinnings of Naive Bayes, where we'll decipher the secrets of Bayes' theorem and the "naive" assumption, and
        weave our way through illustrative examples to unravel its charm.</p>

    <h2>Step 1: Introduce the Problem and Dataset</h2>
    <p>To begin our journey, let's introduce the classification problem at hand and the dataset we'll be using to
        illustrate the magic of Naive Bayes. Our dataset consists of two mystical features (x_1 and x_2) and a binary
        class label (C) determining whether an animal is a "Dog" or a "Cat."</p>
    <table>
        <tr>
            <th>Feature 1 (x_1)</th>
            <th>Feature 2 (x_2)</th>
            <th>Class (C)</th>
        </tr>
        <tr>
            <td>1</td>
            <td>1</td>
            <td>Dog</td>
        </tr>
        <tr>
            <td>1</td>
            <td>0</td>
            <td>Dog</td>
        </tr>
        <tr>
            <td>0</td>
            <td>1</td>
            <td>Cat</td>
        </tr>
        <tr>
            <td>0</td>
            <td>0</td>
            <td>Cat</td>
        </tr>
    </table>
    <br>

    <h2>Step 2: Introduce the Naive Bayes Algorithm</h2>

    <p>Before diving into the enchanting world of mathematics, let's take a moment to understand the Naive Bayes
        algorithm itself. Naive Bayes is a probabilistic algorithm based on Bayes' theorem and the "naive" assumption of
        conditional independence of features given the class label. Despite its simplicity, Naive Bayes has proven to be
        remarkably powerful and finds applications in various fields, including natural language processing, text
        classification, and spam filtering.</p>

    <h2>Step 3: Present Bayes' Theorem - Unraveling the Enigma</h2>

    <p>Behold Bayes' theorem, a magical revelation in probability theory that forms the very essence of Naive Bayes. It
        reveals the posterior probability of an event A given the evidence B, conjured from the product of the prior
        probability of A and the likelihood of observing B given A, all divided by the probability of observing B:</p>

    <pre><code>P(A|B) = P(A) * P(B|A) / P(B)</code></pre>
    <p>Where:</p>
    <ul>
        <li>P(A|B) is the posterior probability of event A occurring given that event B has occurred.</li>
        <li>P(A) is the prior probability of event A, representing our initial belief about the probability of A before
            considering any evidence.</li>
        <li>P(B|A) is the likelihood of observing evidence B given that event A has occurred.</li>
        <li>P(B) is the probability of observing evidence B.</li>
    </ul>
    <br>
    <h2>Step 4: Explain the "Naive" Assumption - A Masterstroke of Simplicity</h2>

    <p>Now, in the context of Naive Bayes, we introduce the "naive" assumption. This assumption states that the features
        (variables) in the dataset are conditionally independent of each other, given the class label. In simple terms,
        it means that the presence or absence of one feature does not influence the presence or absence of another
        feature, given the knowledge of the class label.</p>
    <p>This assumption simplifies the calculations significantly and allows us to express the joint probability of
        observing all features given the class label C as the product of individual probabilities of observing each
        feature given the class label:</p>
    <pre><code>P(x_1, x_2, ..., x_n | C) = P(x_1 | C) * P(x_2 | C) * ... * P(x_n | C)</code></pre>

    <br>
    <h2>Step 5: Introduce the Naive Bayes Equations</h2>

    <p>Now that we've set the stage with Bayes' theorem and the "naive" assumption, let us conjure the magical equations
        of Naive Bayes. Imagine a dataset with an ensemble of features X = {x_1, x_2, ..., x_n}, and a mysterious class
        label C. We can now derive the Naive Bayes equation as follows:</p>

    <pre><code>P(C | X) = P(C) * P(x_1 | C) * P(x_2 | C) * ... * P(x_n | C) / P(X)</code></pre>

    <p>This is the derived Naive Bayes equation! It allows us to calculate the posterior probability of a class label
        given a set of features and use it to make predictions for new instances.</p>

    <br>
    <h2>Step 6: Applying the Naive Bayes Equation</h2>

    <p>In practice, to make predictions, we calculate the posterior probabilities for each class label, and the class
        with the highest probability becomes our predicted class for the new instance.</p>

    <h3>Example Calculation</h3>
    <p>Let's illustrate Naive Bayes with a simple example. Suppose we have a dataset with two features (x_1 and x_2) and
        a binary class label (C) indicating whether an animal is a "Dog" or a "Cat." We will use the following training
        data:</p>
    <table>
        <tr>
            <th>Feature 1 (x_1)</th>
            <th>Feature 2 (x_2)</th>
            <th>Class (C)</th>
        </tr>
        <tr>
            <td>1</td>
            <td>1</td>
            <td>Dog</td>
        </tr>
        <tr>
            <td>1</td>
            <td>0</td>
            <td>Dog</td>
        </tr>
        <tr>
            <td>0</td>
            <td>1</td>
            <td>Cat</td>
        </tr>
        <tr>
            <td>0</td>
            <td>0</td>
            <td>Cat</td>
        </tr>
    </table>
    <p>Now, let's calculate the probabilities required for Naive Bayes to predict the class of a new instance with
        features X_new = {x_1=1, x_2=1}.</p>

    <br>
    <h3>Step 1: Calculate the prior probabilities P(Dog) and P(Cat):</h3>

    <pre><code>P(Dog) = Number of instances of Dog / Total number of instances = 2 / 4 = 0.5</code></pre>
    <pre><code>P(Cat) = Number of instances of Cat / Total number of instances = 2 / 4 = 0.5</code></pre>
    <br>

    <h3>Step 2: Calculate the conditional probabilities P(x_i|C):</h3>

    <pre><code>P(x_1=1|Dog) = Number of instances of Dog with x_1=1 / Total number of instances of Dog = 2 / 2 = 1</code></pre>
    <pre><code>P(x_2=1|Dog) = Number of instances of Dog with x_2=1 / Total number of instances of Dog = 1 / 2 = 0.5</code></pre>
    <pre><code>P(x_1=1|Cat) = Number of instances of Cat with x_1=1 / Total number of instances of Cat = 0 / 2 = 0</code></pre>
    <pre><code>P(x_2=1|Cat) = Number of instances of Cat with x_2=1 / Total number of instances of Cat = 1 / 2 = 0.5</code></pre>

    <br>

    <h3>Step 3: Calculate the likelihood of the new instance P(X_new|C):</h3>

    <pre><code>P(X_new|Dog) = P(x_1=1|Dog) * P(x_2=1|Dog) = 1 * 0.5 = 0.5</code></pre>
    <pre><code>P(X_new|Cat) = P(x_1=1|Cat) * P(x_2=1|Cat) = 0 * 0.5 = 0</code></pre>

    <br>

    <h3>Step 4: Calculate the posterior probabilities P(Dog|X_new) and P(Cat|X_new):</h3>

    <pre><code>P(Dog|X_new) = P(Dog) * P(X_new|Dog) / P(X_new) = 0.5 * 0.5 / P(X_new)</code></pre>
    <pre><code>P(Cat|X_new) = P(Cat) * P(X_new|Cat) / P(X_new) = 0.5 * 0 / P(X_new)</code></pre>

    <br>
    <h3>Step 5: Normalize the probabilities:</h3>
    <p>As P(Dog|X_new) + P(Cat|X_new) = 1, we can normalize the probabilities:</p>
    <pre><code>P(Dog|X_new) = 0.5 / (0.5 + 0) = 1</code></pre>
    <pre><code>P(Cat|X_new) = 0 / (0.5 + 0) = 0</code></pre>
    <br>
    <h2>Conclusion: Embrace the Magic of Naive Bayes</h2>

    <p>As our mystical journey comes to an end, we've unveiled the mathematical charm behind Naive Bayes, illuminated by
        the radiant glow of Bayes' theorem and the "naive" assumption. We've mastered the magical equations and summoned
        its power to predict the unseen. Naive Bayes, a beguiling combination of simplicity and potency, continues to
        weave its magic in the realm of machine learning, enthralling us with its applications in various fields.
        Embrace the magic of Naive Bayes and let it guide you on your own captivating adventures in the enchanted world
        of classification algorithms. May your path be bright, and your predictions ever-accurate!</p>



    <p style="text-align: justify;" class="github-link">
        <a href="" title="Link to GitHub Repository" style="color: #0000FF;">
            <img src="https://git-scm.com/images/logos/downloads/Git-Icon-Black.png" alt="Link to GitHub Repository">
            GitHub Repository
        </a>
    </p>

</body>

</html>