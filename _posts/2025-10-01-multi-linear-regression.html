---
layout: post
title: "Multiple Linear Regression Model"
subtitle: "Mathematical foundations of ML algorithm"
date: 2024-08-26 18:00:18 -0400
background: '/img/posts/04.jpg'
categories: [ML, MLR, LR]
tags: [machine-learning, linear-regression, mathematics, tutorial]
description: "A step by step explanation of multiple linear regression, covering the method, interpretation of coefficients, and how it is applied in real world problems."
---

<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Understanding Linear Regression: A Step-by-Step Guide</title>

    <!-- MathJax for LaTeX rendering -->
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer></script>
    <style>
        /* Page background (kept from original) */
        body {
            background: url('/img/trig.gif') center center / cover no-repeat fixed;
            background-size: cover;
            background-attachment: fixed;
            padding: 0;
            margin: 0;
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial;
            color: #111;
            line-height: 1.6;
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
            /* optional: prevent selection via CSS for modern browsers (original used JS too) */
            -webkit-user-select: none;
            -moz-user-select: none;
            -ms-user-select: none;
            user-select: none;
        }

        /* Content container for better contrast */
        .content {
            background: rgba(255, 255, 255, 0.92);
            max-width: 1000px;
            margin: 30px auto;
            padding: 28px;
            border-radius: 8px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.12);
        }

        h1 {
            margin-top: 0;
            font-size: 30px;
        }

        h2 {
            font-size: 22px;
            margin: 14px 0;
        }

        h3 {
            font-size: 18px;
            margin: 12px 0;
        }

        h4 {
            font-size: 16px;
            margin: 10px 0;
        }

        p {
            text-align: justify;
        }

        code {
            background: #f5f5f5;
            padding: 3px 6px;
            border-radius: 4px;
            font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, "Roboto Mono", "Courier New", monospace;
        }

        table {
            border-collapse: collapse;
            width: 100%;
            margin: 12px 0 22px;
        }

        table th,
        table td {
            border: 1px solid #ddd;
            padding: 10px;
            text-align: center;
        }

        table th {
            background: #f8f8f8;
        }

        .img-fluid {
            display: block;
            max-width: 100%;
            height: auto;
            margin: 14px 0;
        }

        .github-link {
            display: flex;
            align-items: center;
            justify-content: flex-start;
            font-style: italic;
            font-size: 16px;
            margin-top: 20px;
        }

        .github-link img {
            width: 40px;
            height: 40px;
            margin-right: 8px;
        }

        /* Back to top button */
        #myBtn {
            display: none;
            position: fixed;
            bottom: 20px;
            right: 30px;
            z-index: 99;
            font-size: 15px;
            border: none;
            outline: none;
            background-color: rgb(238, 208, 37);
            color: white;
            cursor: pointer;
            padding: 10px;
            border-radius: 4px;
        }

        #myBtn:hover {
            background-color: #555;
        }

        /* Make code blocks responsive */
        pre {
            overflow-x: auto;
            background: #f7f7f7;
            padding: 12px;
            border-radius: 6px;
        }

        /* Put this inside <style> in the <head> */
        .math-display {
            overflow-x: auto !important;
            /* Adds horizontal scroll if still too wide */
            white-space: normal !important;
            word-break: break-word !important;
        }

        mjx-container[jax="CHTML"][display="true"] {
            overflow-x: auto;
            display: block;
            max-width: 100%;
        }


        /* üåô Dark mode styles */
        body.dark-mode {
            background-color: #121212 !important;
            color: #f0f0f0 !important;
        }

        body.dark-mode .content {
            background: rgba(30, 30, 30, 0.92) !important;
            color: #f0f0f0 !important;
        }

        body.dark-mode pre {
            background: #1e1e1e !important;
            color: #f0f0f0 !important;
        }

        /* Toggle button styling */
        .toggle-btn {
            position: fixed;
            top: 20px;
            right: 20px;
            z-index: 9999;
            background: #333;
            color: #fff;
            border: none;
            padding: 10px 16px;
            border-radius: 8px;
            cursor: pointer;
            font-size: 16px;
            transition: all 0.3s ease;
        }

        .toggle-btn:hover {
            background: #555;
        }

        /* Force tables to respect container width */
        .table-wrapper {
            max-width: 100%;
            overflow-x: auto;
            /* horizontal scroll if needed */
            -webkit-overflow-scrolling: touch;
            /* smoother on mobile */
        }

        /* Make the table itself shrink/scroll, not stretch page */
        .table-wrapper table {
            width: max-content;
            /* only as wide as its content */
            min-width: 100%;
            /* but at least fill container width */
            border-collapse: collapse;
        }

        .table-wrapper th,
        .table-wrapper td {
            white-space: nowrap;
            /* prevent numbers from breaking awkwardly */
            padding: 8px;
            border: 1px solid #ddd;
            text-align: center;
        }
    </style>

</head>

<body>
    <!-- üåô Dark Mode Toggle Button -->
    <button id="darkModeToggle" class="toggle-btn">üåô Dark Mode</button>

    <div class="content">
        <script>
            // üåô Dark mode toggle logic
            const toggleBtn = document.getElementById("darkModeToggle");
            toggleBtn.addEventListener("click", () => {
                document.body.classList.toggle("dark-mode");
                toggleBtn.textContent = document.body.classList.contains("dark-mode")
                    ? "‚òÄÔ∏è"
                    : "üåô ";
            });
        </script>

        <button onclick="topFunction()" id="myBtn" title="Back to top" aria-label="Back to top">
            <img class="img-fluid" src="/img/posts/arrow.jpg" height="30" width="30" alt="Back to top">
        </button>
        <!-- ‚úÖ ‚úÖ ‚úÖ BLOGPOST CONTENT STARTS HERE ‚úÖ ‚úÖ ‚úÖ -->

        <h1>Multiple Linear Regression: A Step-by-Step Explanation</h1>

        <p>
            In the previous post on <a
                href="https://pr-peri.github.io/ml/slr/lr/2024/08/25/simple-linear-regression.html"><strong>Simple
                    Linear
                    Regression</strong></a>,
            we saw how a single predictor explains a response variable.
            But in most real problems, one factor is never enough. For house prices, salary prediction, or stock
            returns, multiple
            factors interact simultaneously.
        </p>
        <p>
            This post introduces <strong>Multiple Linear Regression (MLR)</strong>. By the end of this article, you
            will understand:
        </p>
        <ul>
            <li>What MLR is and why it matters</li>
            <li>How the regression equation is derived step by step</li>
            <li>A complete worked example with a dataset</li>
            <li>How to interpret the coefficients in plain language</li>
            <li>Key assumptions, pitfalls, and references for deeper study</li>
        </ul>
        <hr>

        <h2>1. The Idea Behind Multiple Linear Regression</h2>
        <p>
            Multiple Linear Regression extends simple regression by allowing more predictors. Instead of drawing a
            straight line through points on a 2D chart, here we fit a ‚Äúplane‚Äù (or hyperplane in higher dimensions)
            through multi-dimensional data.
        </p>

        <p style="text-align:center;">
            \[ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \varepsilon \]
        </p>

        <ul>
            <li>\(y\): outcome (dependent variable)</li>
            <li>\(x_1, x_2, \dots, x_p\): predictors (independent variables)</li>
            <li>\(\beta_0\): intercept (baseline when predictors are zero)</li>
            <li>\(\beta_j\): slope/partial effect of predictor \(j\)</li>
            <li>\(\varepsilon\): error term (unexplained part)</li>
        </ul>

        <p>
            <strong>Interpretation:</strong> Each coefficient \(\beta_j\) measures the effect of predictor \(x_j\)
            on \(y\), <em>while holding all other predictors constant</em>. This ‚Äúall else being equal‚Äù view is what
            makes MLR powerful. Think of cooking: the final taste depends on many ingredients. To know the effect of
            salt, you must hold oil and spices fixed ‚Äî otherwise you confuse the effects.
        </p>
        <hr>

        <h2>2. Deriving the Equations</h2>

        <h3>2.1 Least Squares Principle</h3>
        <p>
            The main idea is simple: find coefficients so that the predicted values are as close as possible to the
            actual values. We measure closeness using the <strong>sum of squared errors (SSE)</strong>:
        </p>

        <p style="text-align:center;">
            \[ SSE = \sum_{i=1}^n \bigl(y_i - \hat{y}_i\bigr)^2 \]
        </p>

        <p>where the fitted values are:</p>

        <p style="text-align:center;">
            \[ \hat{y}_i = b_0 + b_1 x_{1i} + b_2 x_{2i} \]
        </p>

        <p>
            Minimizing SSE means finding the ‚Äúbest compromise line/plane.‚Äù To do this, we take partial derivatives
            with respect to each coefficient, set them to zero, and solve. This produces the
            <strong>normal equations</strong>.
        </p>

        <p style="text-align:center;">
            \[ \mathbf{b} = (X^\top X)^{-1} X^\top y \]
        </p>

        <p>
            While the matrix formula looks abstract, it‚Äôs just a compact way of solving the system of equations that
            balance predictor‚Äìresponse relationships. For teaching, we usually compute using <em>deviations from the
                mean</em> and cross-products ‚Äî this makes the math more intuitive.
        </p>
        <hr>

        <h2>3. Example Dataset</h2>

        <p>
            To see this in action, consider a small dataset of house prices with two predictors: area and number of
            bedrooms. We expect both to increase price, but how much does each matter when we consider them
            together?
        </p>

        <table>
            <tr>
                <th>House</th>
                <th>Area (sq.ft)</th>
                <th>Bedrooms</th>
                <th>Price (k$)</th>
            </tr>
            <tr>
                <td>1</td>
                <td>1000</td>
                <td>2</td>
                <td>250</td>
            </tr>
            <tr>
                <td>2</td>
                <td>1500</td>
                <td>3</td>
                <td>400</td>
            </tr>
            <tr>
                <td>3</td>
                <td>2000</td>
                <td>4</td>
                <td>450</td>
            </tr>
            <tr>
                <td>4</td>
                <td>2500</td>
                <td>3</td>
                <td>500</td>
            </tr>
            <tr>
                <td>5</td>
                <td>3000</td>
                <td>5</td>
                <td>550</td>
            </tr>
        </table>
        <hr>
        <h2>4. Step-by-Step Solution</h2>

        <h3>Step 1: Compute Means</h3>
        <p>
            First, we find averages of each variable ‚Äî this gives us the ‚Äúcenter‚Äù of the data:
        </p>

        <p style="text-align:center;">
            \[
            \bar{x}_1 = 2000, \quad \bar{x}_2 = 3.4, \quad \bar{y} = 430
            \]
        </p>

        <p>
            These means act as reference points. Every other calculation measures how much each observation deviates
            from these averages.
        </p>
        <hr>

        <h3>Step 2: Deviations</h3>
        <p>
            Subtract the mean from each observation. Deviations tell us whether each house is above or below average.
        </p>

        <p style="text-align:center;">
            \[
            x_{1i}' = x_{1i} - \bar{x}_1, \quad
            x_{2i}' = x_{2i} - \bar{x}_2, \quad
            y_i' = y_i - \bar{y}
            \]
        </p>

        <p>Example (House 1):</p>

        <p style="text-align:center;">
            \[
            x_{11}' = -1000, \quad x_{21}' = -1.4, \quad y_1' = -180
            \]
        </p>

        <p>
            This means House 1 is much smaller and cheaper than the average house, with fewer bedrooms too.
        </p>

        <table>
            <tr>
                <th>House</th>
                <th>x1</th>
                <th>x1 - xÃÑ1</th>
                <th>x2</th>
                <th>x2 - xÃÑ2</th>
                <th>y</th>
                <th>y - »≥</th>
            </tr>
            <tr>
                <td>1</td>
                <td>1000</td>
                <td>-1000</td>
                <td>2</td>
                <td>-1.4</td>
                <td>250</td>
                <td>-180</td>
            </tr>
            <tr>
                <td>2</td>
                <td>1500</td>
                <td>-500</td>
                <td>3</td>
                <td>-0.4</td>
                <td>400</td>
                <td>-30</td>
            </tr>
            <tr>
                <td>3</td>
                <td>2000</td>
                <td>0</td>
                <td>4</td>
                <td>+0.6</td>
                <td>450</td>
                <td>+20</td>
            </tr>
            <tr>
                <td>4</td>
                <td>2500</td>
                <td>+500</td>
                <td>3</td>
                <td>-0.4</td>
                <td>500</td>
                <td>+70</td>
            </tr>
            <tr>
                <td>5</td>
                <td>3000</td>
                <td>+1000</td>
                <td>5</td>
                <td>+1.6</td>
                <td>550</td>
                <td>+120</td>
            </tr>
        </table>
        <hr>
        <h3>Step 3: Cross-Products</h3>
        <p>
            At this stage, we already have the deviations (how far each value is from its mean). But before jumping to
            deviations, let‚Äôs first see how the <em>raw equations</em> look. They come directly from minimizing the sum
            of
            squared errors.
        </p>

        <h4>3.1 Least Squares Setup</h4>
        <p>
            Regression model:
        </p>

        <p style="text-align:center;">
            \[
            y_i = b_0 + b_1 x_{1i} + b_2 x_{2i} + e_i
            \]
        </p>

        <p>
            We minimize:
        </p>

        <p style="text-align:center;">
            \[
            SSE = \sum_{i=1}^n (y_i - b_0 - b_1 x_{1i} - b_2 x_{2i})^2
            \]
        </p>

        <h4>3.2 First-Order Conditions</h4>
        <p>
            Setting derivatives equal to zero gives three equations:
        </p>

        <p style="text-align:center;">
            \[
            \sum (y_i - b_0 - b_1 x_{1i} - b_2 x_{2i}) = 0
            \]
            \[
            \sum x_{1i}(y_i - b_0 - b_1 x_{1i} - b_2 x_{2i}) = 0
            \]
            \[
            \sum x_{2i}(y_i - b_0 - b_1 x_{1i} - b_2 x_{2i}) = 0
            \]
        </p>

        <h4>3.3 Expanded Normal Equations</h4>
        <p>
            Expanding these yields the <strong>normal equations in raw form</strong>:
        </p>

        <p style="text-align:center;">
            \[
            \sum y_i = nb_0 + b_1 \sum x_{1i} + b_2 \sum x_{2i}
            \]
            \[
            \sum x_{1i}y_i = b_0 \sum x_{1i} + b_1 \sum x_{1i}^2 + b_2 \sum (x_{1i}x_{2i})
            \]
            \[
            \sum x_{2i}y_i = b_0 \sum x_{2i} + b_1 \sum (x_{1i}x_{2i}) + b_2 \sum x_{2i}^2
            \]
        </p>

        <h4>3.4 Switch to Deviation Form</h4>
        <p>
            To simplify, we subtract out means. For example:
        </p>

        <p style="text-align:center;">
            \[
            \sum (x_{1i}y_i) - \frac{\sum x_{1i}\sum y_i}{n} = \sum (x_{1i}'y_i')
            \]
        </p>

        <p>
            This conversion removes \(b_0\) from the middle equations and makes everything centered around averages.
            That‚Äôs why we define the following cross-products:
        </p>

        <p style="text-align:center;">
            \[
            S_{x_1x_1} = \sum (x_{1i}')^2, \quad
            S_{x_2x_2} = \sum (x_{2i}')^2, \quad
            S_{x_1y} = \sum (x_{1i}'y_i')
            \]
            \[
            S_{x_2y} = \sum (x_{2i}'y_i'), \quad
            S_{x_1x_2} = \sum (x_{1i}'x_{2i}')
            \]
        </p>

        <p>
            Each entry is just multiplying deviations row by row. Example (House 1):
        </p>

        <p style="text-align:center;">
            \[
            (x_1' \text{ for House 1}) \times (y' \text{ for House 1})
            = (-1000)(-180) = 180{,}000
            \]
        </p>

        <p>
            Since both values are below average, their product is positive, showing that area and price move together.
        </p>

        <table>
            <tr>
                <td>1</td>
                <td>1,000,000</td>
                <td>1.96</td>
                <td>180,000</td>
                <td>252</td>
                <td>1400</td>
            </tr>
            <tr>
                <td>2</td>
                <td>250,000</td>
                <td>0.16</td>
                <td>15,000</td>
                <td>12</td>
                <td>200</td>
            </tr>
            <tr>
                <td>3</td>
                <td>0</td>
                <td>0.36</td>
                <td>0</td>
                <td>12</td>
                <td>0</td>
            </tr>
            <tr>
                <td>4</td>
                <td>250,000</td>
                <td>0.16</td>
                <td>35,000</td>
                <td>-28</td>
                <td>-200</td>
            </tr>
            <tr>
                <td>5</td>
                <td>1,000,000</td>
                <td>2.56</td>
                <td>120,000</td>
                <td>192</td>
                <td>1600</td>
            </tr>
            <tr>
                <th>Sum</th>
                <th>2,500,000</th>
                <th>5.2</th>
                <th>350,000</th>
                <th>440</th>
                <th>3000</th>
            </tr>

        </table>

        <hr>
        <hr>

        <h3>Step 4: Normal Equations</h3>
        <p>
            Using the corrected cross-product sums, the centered normal equations become:
        </p>

        <p style="text-align:center;">
            \[
            2{,}500{,}000 b_1 + 3000 b_2 = 350{,}000
            \]
            \[
            3000 b_1 + 5.2 b_2 = 440
            \]
        </p>
        <hr>

        <h3>Step 5: Solve</h3>
        <p>
            Solving this system step by step, we get:
        </p>

        <p style="text-align:center;">
            \[
            b_1 \approx 0.125, \quad b_2 \approx 12.5
            \]
        </p>

        <p>Interpretation: each sq.ft adds about $125 to price, and each additional bedroom adds about $12,500, holding
            area constant.</p>
        <hr>

        <h3>Step 6: Intercept</h3>
        <p>
            Finally, we calculate the intercept ‚Äî the baseline price when predictors are zero
            (not realistic but mathematically required):
        </p>

        <p style="text-align:center;">
            \[
            b_0 = 430 - (0.125)(2000) - (12.5)(3.4) \approx 137.5
            \]
        </p>

        <p><strong>Final regression equation:</strong></p>

        <p style="text-align:center; font-size:1.1em; font-weight:bold;">
            \[
            \hat{y} = 137.5 + 0.125 x_1 + 12.5 x_2
            \]
        </p>
        <hr>
        <!-- ‚úÖ NEW ADDITION: Visualization -->
        <h3>Visualizing the Regression Plane</h3>
        <p>
            To better understand what this model represents, let‚Äôs plot the observed data points (houses) along with the
            estimated regression plane. Since we have two predictors (area and bedrooms), the fitted model forms a
            <strong>plane</strong> in 3D space. Each house sits as a blue dot, and the red plane shows how the
            regression
            line balances through the cloud of points.
        </p>

        <p style="text-align:center;">
            <img src="/img/posts/lgr1-post/multiple_linear_regression_plane.png"
                alt="3D regression plane for house price prediction"
                style="max-width:90%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:2px 2px 8px rgba(0,0,0,0.1);" />
        </p>

        <p>
            This visualization helps us see the combined effect: as <em>area</em> and <em>bedrooms</em> increase,
            the regression plane rises, predicting a higher house price. Unlike a simple regression line, the surface
            shows how multiple predictors work together simultaneously.
        </p>
        <hr>

        <h2>5. Prediction Example</h2>
        <p>
            Suppose we want to predict the price of a 2200 sq.ft house with 4 bedrooms:
        </p>

        <p style="text-align:center;">
            \[
            \hat{y} = 137.5 + 0.125(2200) + 12.5(4) = 137.5 + 275 + 50 = 462.5 \ \text{(k\$)} \]
        </p>

        <p>
            So, our model estimates this house would cost around 463k. This result combines both area and bedroom
            contributions rather than looking at one factor in isolation.
        </p>
        <hr>

        <h2>6. Assumptions & Pitfalls</h2>
        <ul>
            <li><strong>Linearity:</strong> Predictors are assumed to have straight-line effects on the outcome. If the
                true relationship is curved, predictions will be off.</li>
            <li><strong>No multicollinearity:</strong> Predictors should not be highly correlated. Otherwise, it‚Äôs hard
                to separate their individual effects.</li>
            <li><strong>Homoscedasticity:</strong> The spread of errors should be constant. If large houses show more
                unpredictable prices, this assumption breaks.</li>
            <li><strong>Independence:</strong> Each observation (house) should be independent. Prices influenced by
                neighborhood clustering may violate this.</li>
            <li><strong>Normality (for inference):</strong> Residuals should roughly follow a normal distribution if we
                want valid confidence intervals and hypothesis tests.</li>
        </ul>
        <hr>

        <h2>Conclusion</h2>
        <p>
            Multiple Linear Regression extends the one-predictor case by balancing contributions from multiple
            variables. We walked through the derivation, solved an example by hand, and saw how to interpret the
            coefficients.
        </p>
        <p>
            Despite its simplicity, MLR remains one of the most used tools in statistics and machine learning because
            it is interpretable, flexible, and applicable across fields ‚Äî from economics to healthcare.
        </p>
        <hr>

        <h2>References</h2>
        <ul>
            <li><a href="https://www.wiley.com/en-us/Introduction+to+Linear+Regression+Analysis%2C+6th+Edition-p-9781119714704"
                    target="_blank">Montgomery, D.C., Peck, E.A., & Vining, G.G. (2021). <em>Introduction to Linear
                        Regression Analysis</em> (6th ed.). Wiley.</a></li>
            <li><a href="https://www.statlearning.com/" target="_blank">James, G., Witten, D., Hastie, T., & Tibshirani,
                    R. (2021). <em>An Introduction to Statistical Learning</em> (2nd ed.). Springer (free online).</a>
            </li>
            <li><a href="https://www.wiley.com/en-us/Applied+Linear+Regression%2C+3rd+Edition-p-9780471704085"
                    target="_blank">Weisberg, S. (2005). <em>Applied Linear Regression</em> (3rd ed.). Wiley.</a></li>
        </ul>
        <!-- ‚úÖ ‚úÖ ‚úÖ BLOGPOST CONTENT END HERE ‚úÖ ‚úÖ ‚úÖ -->
        <p style="text-align: left;" class="github-link">
            <a href="https://github.com/PR-Peri/Linear-Regression-Model-From-Scratch/" title="Link to GitHub Repository"
                style="color: #0000FF; text-decoration: none;">
                <img src="https://git-scm.com/images/logos/downloads/Git-Icon-Black.png" alt="Git icon">
                GitHub Repository
            </a>
        </p>
        {% include related-article.html %}
    </div>
    <script>
        // Message shown on right-click (keeps original behavior)
        const message = "Right- Click Function Disabled!";

        // Right-click: alert message and prevent default context menu
        document.addEventListener('contextmenu', function (e) {
            try { alert(message); } catch (err) { /* fallback: no-op */ }
            e.preventDefault();
        });

        // Prevent certain dev-key combos (F12, Ctrl+Shift+I/C/J, Ctrl+U)
        document.addEventListener('keydown', function (e) {
            // Normalize key for comparisons
            const key = (e.key || '').toUpperCase();

            // F12
            if (e.key === 'F12' || e.keyCode === 123) {
                e.preventDefault();
                return false;
            }

            // Ctrl+Shift+(I/J/C)
            if (e.ctrlKey && e.shiftKey && (key === 'I' || key === 'J' || key === 'C')) {
                e.preventDefault();
                return false;
            }

            // Ctrl+U (view-source / page source)
            if (e.ctrlKey && key === 'U') {
                e.preventDefault();
                return false;
            }
        });

        // Disable text selection and most mouse-driven selection actions (older-browsers compatibility)
        function disableselect(e) { return false; }
        function reEnable() { return true; }

        // Legacy handlers retained for compatibility (keeps original intent)
        document.onselectstart = disableselect; // IE/Old browsers
        if (window.sidebar) { // Netscape / old browsers
            document.onmousedown = disableselect;
            document.onclick = reEnable;
        }

        // Back-to-top button logic
        const mybutton = document.getElementById("myBtn");
        window.onscroll = function () { scrollFunction(); };

        function scrollFunction() {
            if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
                mybutton.style.display = "block";
            } else {
                mybutton.style.display = "none";
            }
        }

        function topFunction() {
            document.body.scrollTop = 0;
            document.documentElement.scrollTop = 0;
        }
    </script>
</body>

</html>