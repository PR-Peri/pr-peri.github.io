---
layout: post
title: "A Beginner‚Äôs Guide to Lasso Regression (L1 Regularization)"
subtitle: "Regularization Techniques I"
date: 2025-10-03 03:05:13 -0400
background: '/img/posts/ridge-lasso/ridge-02.jpg'
categories: [machine-learning]
tags: [machine-learning, regression]
description: "Learn what Lasso Regression is, why it matters, and how it works step by step with simple examples and
explanations."
---


<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Understanding : A Step-by-Step Guide</title>


    <!-- MathJax for LaTeX rendering -->
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer></script>
    <style>
        /* Page background (kept from original) */
        body {
            background: url('/img/trig.gif') center center / cover no-repeat fixed;
            background-size: cover;
            background-attachment: fixed;
            padding: 0;
            margin: 0;
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial;
            color: #111;
            line-height: 1.6;
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
            /* optional: prevent selection via CSS for modern browsers (original used JS too) */
            -webkit-user-select: none;
            -moz-user-select: none;
            -ms-user-select: none;
            user-select: none;
        }

        /* Content container for better contrast */
        .content {
            background: rgba(255, 255, 255, 0.92);
            max-width: 1000px;
            margin: 30px auto;
            padding: 28px;
            border-radius: 8px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.12);
        }

        h1 {
            margin-top: 0;
            font-size: 30px;
        }

        h2 {
            font-size: 22px;
            margin: 14px 0;
        }

        h3 {
            font-size: 18px;
            margin: 12px 0;
        }

        h4 {
            font-size: 16px;
            margin: 10px 0;
        }

        p {
            text-align: justify;
        }

        code {
            background: #f5f5f5;
            padding: 3px 6px;
            border-radius: 4px;
            font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, "Roboto Mono", "Courier New", monospace;
        }

        .table-responsive {
            overflow-x: auto;
            /* Adds horizontal scroll on small screens */
            -webkit-overflow-scrolling: touch;
            /* Smooth scrolling on iOS */
        }

        .table-responsive table {
            width: 100%;
            /* Keep table width 100% of the container */
            min-width: 600px;
            /* Optional: prevents columns from squishing too much */
        }

        table {
            border-collapse: collapse;
            width: 100%;
            margin: 12px 0 22px;
        }

        table th,
        table td {
            border: 1px solid #ddd;
            padding: 10px;
            text-align: center;
        }

        table th {
            background: #f8f8f8;
        }

        .img-fluid {
            display: block;
            max-width: 100%;
            height: auto;
            margin: 14px 0;
        }

        .github-link {
            display: flex;
            align-items: center;
            justify-content: flex-start;
            font-style: italic;
            font-size: 16px;
            margin-top: 20px;
        }

        .github-link img {
            width: 40px;
            height: 40px;
            margin-right: 8px;
        }

        /* Back to top button */
        #myBtn {
            display: none;
            position: fixed;
            bottom: 20px;
            right: 30px;
            z-index: 99;
            font-size: 15px;
            border: none;
            outline: none;
            background-color: rgb(238, 208, 37);
            color: white;
            cursor: pointer;
            padding: 10px;
            border-radius: 4px;
        }

        #myBtn:hover {
            background-color: #555;
        }

        /* Make code blocks responsive */
        pre {
            overflow-x: auto;
            background: #f7f7f7;
            padding: 12px;
            border-radius: 6px;
        }

        /* Put this inside <style> in the <head> */
        .math-display {
            overflow-x: auto !important;
            /* Adds horizontal scroll if still too wide */
            white-space: normal !important;
            word-break: break-word !important;
        }

        mjx-container[jax="CHTML"][display="true"] {
            overflow-x: auto;
            display: block;
            max-width: 100%;
        }

        @media (max-width: 900px) {
            #darkModeToggle {
                position: fixed;
                /* keep it fixed */
                top: 60px;
                /* slightly lower than the menu button */
                right: 20px;
                /* distance from right edge */
                z-index: 2000;
                /* above menu */
            }
        }

        /* üåô Dark mode styles */
        body.dark-mode {
            background-color: #121212 !important;
            color: #f0f0f0 !important;
        }

        body.dark-mode .content {
            background: rgba(30, 30, 30, 0.92) !important;
            color: #f0f0f0 !important;
        }

        body.dark-mode pre {
            background: #1e1e1e !important;
            color: #f0f0f0 !important;
        }

        /* Toggle button styling */
        .toggle-btn {
            position: fixed;
            top: 20px;
            right: 20px;
            z-index: 9999;
            background: #333;
            color: #fff;
            border: none;
            padding: 10px 16px;
            border-radius: 8px;
            cursor: pointer;
            font-size: 16px;
            transition: all 0.3s ease;
        }

        .toggle-btn:hover {
            background: #555;
        }
    </style>

</head>

<body>
    <!-- üåô Dark Mode Toggle Button -->
    <button id="darkModeToggle" class="toggle-btn">üåô</button>

    <div class="content">
        <script>
            // üåô Dark mode toggle logic
            const toggleBtn = document.getElementById("darkModeToggle");
            toggleBtn.addEventListener("click", () => {
                document.body.classList.toggle("dark-mode");
                toggleBtn.textContent = document.body.classList.contains("dark-mode")
                    ? "‚òÄÔ∏è"
                    : "üåô ";
            });
        </script>

        <button onclick="topFunction()" id="myBtn" title="Back to top" aria-label="Back to top">
            <img class="img-fluid" src="/img/posts/arrow.jpg" height="30" width="30" alt="Back to top">
        </button>


        <!-- ‚úÖ BLOGPOST CONTENT STARTS -->
        <h1>A Beginner‚Äôs Guide to Lasso Regression</h1>

        <h2>1. Introduction</h2>
        <p>
            In previous posts, we explored <strong>Linear Regression</strong>, <strong>Ridge Regression</strong>, and
            evaluation metrics like RMSE and R¬≤.
            In this post, we‚Äôll dive into <strong>Lasso Regression</strong>, short for <em>Least Absolute Shrinkage and
                Selection Operator</em>.
        </p>
        <p>
            While Ridge Regression (L2 penalty) shrinks coefficients but keeps them nonzero, <strong>Lasso Regression
                (L1 penalty) can shrink some coefficients to exactly zero</strong>.
            This makes Lasso particularly powerful for <strong>feature selection</strong>, since it automatically drops
            irrelevant predictors.
        </p>

        <h2>2. The Lasso Cost Function</h2>
        <p>Ordinary Least Squares (OLS) minimizes squared errors:</p>
        <p class="math-display">$$ J(\beta) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$</p>
        <p>Ridge adds an L2 penalty:</p>
        <p class="math-display">$$ J_{ridge}(\beta) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^p
            \beta_j^2 $$</p>
        <p>Lasso instead uses an L1 penalty:</p>
        <p class="math-display">$$ J_{lasso}(\beta) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^p
            |\beta_j| $$</p>

        <h2>3. Why Does Lasso Set Coefficients to Zero?</h2>
        <p>
            Geometrically, Ridge uses a circular (L2 ball) constraint, while Lasso uses a diamond-shaped (L1 ball)
            constraint.
            The corners of the diamond tend to align with axes, meaning solutions often hit an axis, forcing
            coefficients to zero.
        </p>
        <img src="/img/posts/ridge-lasso/lasso-geometry.png" alt="Lasso vs Ridge geometry" class="img-fluid">
        <p><em>Explanation:</em> Notice how the sharp corners of the diamond (L1) are more likely to ‚Äútouch‚Äù the optimal
            solution on an axis, forcing coefficients to exactly zero. Ridge‚Äôs circle (L2) only shrinks them but rarely
            sets them to zero.</p>

        <h3>Soft-thresholding</h3>
        <p>
            The Lasso solution is based on a process called <strong>soft-thresholding</strong>:
        </p>
        <p class="math-display">$$ S(z, \gamma) = \text{sign}(z) \cdot \max(|z| - \gamma, 0) $$</p>
        <p>
            This pushes small coefficient values to exactly zero. Imagine a V-shaped function: flat at the bottom
            (coefficients vanish), slanted outside (coefficients shrink).
        </p>
        <img src="/img/posts/ridge-lasso/lasso_soft_threshold.png" alt="Soft-thresholding function" class="img-fluid">
        <p><em>Explanation:</em> Any coefficient smaller than Œ≥ in magnitude becomes exactly zero. This is how Lasso
            performs automatic feature selection.</p>

        <h2>4. Example with a Tiny Dataset</h2>
        <p>Let‚Äôs use a small dataset so we can follow the math step by step:</p>
        <table>
            <tr>
                <th>X</th>
                <th>y</th>
            </tr>
            <tr>
                <td>1</td>
                <td>2</td>
            </tr>
            <tr>
                <td>2</td>
                <td>3</td>
            </tr>
            <tr>
                <td>3</td>
                <td>5</td>
            </tr>
            <tr>
                <td>4</td>
                <td>7</td>
            </tr>
        </table>

        <img src="/img/posts/ridge-lasso/lasso_scatter_ols.png" alt="OLS regression line on dataset" class="img-fluid">

        <p>The OLS solution gives approximately:</p>
        <p class="math-display">$$ y = 0.5 + 1.5x $$</p>

        <h3>Step 1: Compute z</h3>
        <p>We compute correlation between X and y:</p>
        <p class="math-display">$$ z = \frac{1}{n} \sum x_i y_i = \frac{1}{4}(1*2 + 2*3 + 3*5 + 4*7) = 12.75 $$</p>

        <h3>Step 2: Apply Penalty</h3>
        <p>With Œª = 1:</p>
        <p class="math-display">$$ \gamma = \frac{\lambda}{2n} = \frac{1}{8} = 0.125 $$</p>

        <h3>Step 3: Soft-thresholding</h3>
        <p class="math-display">$$ \beta_1 = S(12.75, 0.125) = 12.625 $$</p>
        <p>The coefficient shrinks slightly. Larger Œª shrinks more, eventually reaching zero.</p>

        <h3>Step 4: Manual Python Demo</h3>
        <pre><code class="language-python">
import numpy as np

X = np.array([1,2,3,4])
y = np.array([2,3,5,7])
n = len(y)

z = (1/n) * np.sum(X * y)
lam = 1
gamma = lam / (2*n)

def soft_threshold(z, gamma):
    if z > gamma:
        return z - gamma
    elif z < -gamma:
        return z + gamma
    else:
        return 0

beta1 = soft_threshold(z, gamma)
print("z =", z)
print("gamma =", gamma)
print("Updated coefficient Œ≤1 =", beta1)
print("Predictions:", beta1 * X)
        </code></pre>

        <h2>5. Using Scikit-learn</h2>
        <pre><code class="language-python">
from sklearn.linear_model import Lasso
import numpy as np
from sklearn.preprocessing import StandardScaler

X = np.array([[1],[2],[3],[4]])
y = np.array([2,3,5,7])

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

model = Lasso(alpha=1)
model.fit(X_scaled,y)

print("Intercept:", model.intercept_)
print("Coefficient:", model.coef_)
        </code></pre>
        <p><em>Explanation:</em> Unlike the manual demo, scikit-learn handles iterative updates and bias term. Notice
            that coefficients may shrink to zero depending on Œª.</p>

        <h2>6. Visualizations</h2>
        <img src="/img/posts/ridge-lasso/lasso_lines.png" alt="Lasso coefficient path" class="img-fluid">
        <p><em>Explanation:</em> Each colored line is a coefficient path. As Œª increases (moving right), more
            coefficients are pulled to zero. This demonstrates Lasso‚Äôs ability to perform feature selection.</p>

        <img src="/img/posts/ridge-lasso/lasso_lines.png" alt="Error vs lambda in Lasso" class="img-fluid">
        <p><em>Explanation:</em> Training error always rises as Œª increases (model becomes simpler). Validation error
            often decreases first (reducing overfitting), then rises again (underfitting). The ‚ÄúU-shape‚Äù shows why
            cross-validation is needed to find the optimal Œª.</p>

        <h2>7. Ridge vs Lasso</h2>
        <table>
            <tr>
                <th>Aspect</th>
                <th>Ridge</th>
                <th>Lasso</th>
            </tr>
            <tr>
                <td>Penalty</td>
                <td>L2 (squared)</td>
                <td>L1 (absolute)</td>
            </tr>
            <tr>
                <td>Effect on Coefficients</td>
                <td>Shrinks but never zero</td>
                <td>Can shrink to exactly zero</td>
            </tr>
            <tr>
                <td>Use Case</td>
                <td>Handles multicollinearity</td>
                <td>Feature selection</td>
            </tr>
        </table>

        <h2>8. Key Takeaways</h2>
        <ul>
            <li>Lasso adds an L1 penalty, unlike Ridge which uses L2.</li>
            <li>Lasso can shrink coefficients to zero, performing automatic feature selection.</li>
            <li>Always standardize features before applying Lasso.</li>
            <li>The choice of Œª (alpha) is crucial: too small ‚Üí overfitting, too large ‚Üí underfitting.</li>
        </ul>

        <h2>9. Conclusion</h2>
        <p>
            Lasso Regression is a powerful extension of linear regression.
            It combats overfitting through regularization, and simplifies models by removing irrelevant features.
            <strong>Ridge shrinks, Lasso selects.</strong>
        </p>

        <h2>10. References</h2>
        <ul>
            <li>Tibshirani, R. (1996). Regression Shrinkage and Selection via the Lasso.</li>
            <li><a href="https://scikit-learn.org/stable/modules/linear_model.html#lasso">Scikit-learn Lasso Docs</a>
            </li>
            <li>Elements of Statistical Learning (Hastie, Tibshirani, Friedman)</li>
        </ul>

        <!-- ‚úÖ BLOGPOST CONTENT ENDS -->
        {% include related-article.html %}
    </div>
    <script src="/includes/blog-style.js"></script>
</body>

</html>