---
layout: post
title: "A Beginner‚Äôs Guide to Ridge Regression (L2 Regularization)"
subtitle:  "Regularization Techniques I"
date: 2025-10-02 19:05:13 -0400
background: '/img/posts/ridge-post/ridge-01.jpg'
categories: [ML,LR]
tags: [machine-learning, regression]
description: "Learn Ridge Regression (L2 regularization) step by step with beginner-friendly explanations, manual calculations, derivations, Python code, and visualizations."
---

<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Understanding Ridge Regression (L2 Regularization)</title>

    <!-- MathJax for LaTeX rendering -->
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer></script>
    <style>
        /* Page background */
        body {
            background: url('/img/trig.gif') center center / cover no-repeat fixed;
            background-size: cover;
            background-attachment: fixed;
            padding: 0;
            margin: 0;
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial;
            color: #111;
            line-height: 1.6;
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
            -webkit-user-select: none;
            -moz-user-select: none;
            -ms-user-select: none;
            user-select: none;
        }

        .content {
            background: rgba(255, 255, 255, 0.92);
            max-width: 1000px;
            margin: 30px auto;
            padding: 28px;
            border-radius: 8px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.12);
        }

        h1 {
            margin-top: 0;
            font-size: 30px;
        }

        h2 {
            font-size: 22px;
            margin: 14px 0;
        }

        h3 {
            font-size: 18px;
            margin: 12px 0;
        }

        h4 {
            font-size: 16px;
            margin: 10px 0;
        }

        p {
            text-align: justify;
        }

        code {
            background: #f5f5f5;
            padding: 3px 6px;
            border-radius: 4px;
            font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, "Roboto Mono", "Courier New", monospace;
        }

        table {
            border-collapse: collapse;
            width: 100%;
            margin: 12px 0 22px;
        }

        table th,
        table td {
            border: 1px solid #ddd;
            padding: 10px;
            text-align: center;
        }

        table th {
            background: #f8f8f8;
        }

        .img-fluid {
            display: block;
            max-width: 100%;
            height: auto;
            margin: 14px 0;
        }

        pre {
            overflow-x: auto;
            background: #f7f7f7;
            padding: 12px;
            border-radius: 6px;
        }

        .math-display {
            overflow-x: auto !important;
            white-space: normal !important;
            word-break: break-word !important;
        }

        mjx-container[jax="CHTML"][display="true"] {
            overflow-x: auto;
            display: block;
            max-width: 100%;
        }

        /* Dark mode */
        body.dark-mode {
            background-color: #121212 !important;
            color: #f0f0f0 !important;
        }

        body.dark-mode .content {
            background: rgba(30, 30, 30, 0.92) !important;
            color: #f0f0f0 !important;
        }

        body.dark-mode pre {
            background: #1e1e1e !important;
            color: #f0f0f0 !important;
        }

        .toggle-btn {
            position: fixed;
            top: 20px;
            right: 20px;
            z-index: 9999;
            background: #333;
            color: #fff;
            border: none;
            padding: 10px 16px;
            border-radius: 8px;
            cursor: pointer;
            font-size: 16px;
            transition: all 0.3s ease;
        }

        .toggle-btn:hover {
            background: #555;
        }
    </style>
</head>

<body>
    <!-- üåô Dark Mode Toggle Button -->
    <button id="darkModeToggle" class="toggle-btn">üåô Dark Mode</button>

    <div class="content">
        <script>
            // üåô Dark mode toggle logic
            const toggleBtn = document.getElementById("darkModeToggle");
            toggleBtn.addEventListener("click", () => {
                document.body.classList.toggle("dark-mode");
                toggleBtn.textContent = document.body.classList.contains("dark-mode")
                    ? "‚òÄÔ∏è"
                    : "üåô ";
            });
        </script>

        <button onclick="topFunction()" id="myBtn" title="Back to top" aria-label="Back to top">
            <img class="img-fluid" src="/img/posts/arrow.jpg" height="30" width="30" alt="Back to top">
        </button>
        <div class="content">

            <!-- ‚úÖ BLOGPOST CONTENT STARTS -->

            <h1>A Beginner‚Äôs Guide to Ridge Regression (L2 Regularization)</h1>

            <p>
                Linear regression is one of the simplest and most powerful tools in machine learning, but it comes with
                weaknesses.
                When predictors are <strong>highly correlated</strong> (a situation called <em>multicollinearity</em>),
                or
                when the dataset is small and noisy,
                the estimated coefficients can become <strong>unstable and extremely large</strong>.
                This instability leads to poor generalization on new data.
                To overcome this, we use <strong>regularization</strong> techniques.
                One of the most popular is <strong>Ridge Regression</strong>, also known as <strong>L2
                    regularization</strong>.
            </p>

            <hr>

            <h2>1. Why Do We Need Ridge Regression?</h2>
            <p>
                Ordinary Least Squares (OLS) regression estimates coefficients by minimizing the sum of squared errors:
            </p>

            <p class="math-display">\[
                J(\beta) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
                \]</p>

            <p>
                This works well when predictors are independent and the dataset is large.
                But when predictors are correlated, \(X^T X\) (the covariance matrix of features) becomes nearly
                singular.
                <strong>Singular means the matrix cannot be inverted</strong>, which makes solving the regression
                unstable.
                Small changes in data can then cause large swings in coefficients.
            </p>

            <p>
                Ridge Regression fixes this by adding a <strong>penalty term</strong> on the size of the coefficients:
            </p>

            <p class="math-display">\[
                J(\beta) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} \beta_j^2
                \]</p>

            <p>
                Here, \( \lambda \) is a tuning parameter:
            </p>
            <ul>
                <li>\(\lambda = 0\): Ridge = OLS (no regularization).</li>
                <li>\(\lambda > 0\): coefficients are shrunk toward zero.</li>
                <li>\(\lambda \to \infty\): all coefficients go toward zero, leading to underfitting.</li>
            </ul>

            <hr>

            <h2>2. How Do We Derive the Ridge Equation?</h2>
            <p>
                The OLS closed-form solution is:
            </p>

            <p class="math-display">\[
                \hat{\beta}_{OLS} = (X^T X)^{-1} X^T y
                \]</p>

            <p>
                For Ridge, the cost function includes the penalty:
            </p>

            <p class="math-display">\[
                J(\beta) = (y - X\beta)^T (y - X\beta) + \lambda \beta^T \beta
                \]</p>

            <p>
                Taking the derivative and setting it to zero gives:
            </p>

            <p class="math-display">\[
                (X^T X + \lambda I)\beta = X^T y
                \]</p>

            <p>
                And solving:
            </p>

            <p class="math-display">\[
                \hat{\beta}_{ridge} = (X^T X + \lambda I)^{-1} X^T y
                \]</p>

            <p>
                This ensures the matrix is invertible and stabilizes the solution.
            </p>

            <hr>

            <h2>3. Manual Example with a Tiny Dataset (Step-by-Step)</h2>
            <p>Dataset:</p>

            <table>
                <thead>
                    <tr>
                        <th>x</th>
                        <th>y</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>1</td>
                        <td>2</td>
                    </tr>
                    <tr>
                        <td>2</td>
                        <td>3</td>
                    </tr>
                    <tr>
                        <td>3</td>
                        <td>5</td>
                    </tr>
                    <tr>
                        <td>4</td>
                        <td>7</td>
                    </tr>
                </tbody>
            </table>

            <p><strong>Step 1: Design matrix \(X\) and response \(y\):</strong></p>

            <p class="math-display">\[
                X = \begin{bmatrix}1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{bmatrix}, \quad
                y = \begin{bmatrix}2 \\ 3 \\ 5 \\ 7\end{bmatrix}
                \]</p>

            <p><strong>Step 2: Compute \(X^T X\) and \(X^T y\):</strong></p>

            <p class="math-display">\[
                X^T X = \begin{bmatrix}4 & 10 \\ 10 & 30\end{bmatrix}, \quad
                X^T y = \begin{bmatrix}17 \\ 51\end{bmatrix}
                \]</p>

            <p><strong>Step 3: Add penalty (\(\lambda = 1\)):</strong></p>

            <p class="math-display">\[
                X^T X + \lambda I = \begin{bmatrix}5 & 10 \\ 10 & 31\end{bmatrix}
                \]</p>

            <p><strong>Step 4: Inverse:</strong></p>

            <p class="math-display">\[
                (X^T X + \lambda I)^{-1} = \frac{1}{55}\begin{bmatrix}31 & -10 \\ -10 & 5\end{bmatrix}
                \]</p>

            <p><strong>Step 5: Multiply (showing the intermediate step):</strong></p>

            <p class="math-display">\[
                \begin{bmatrix}31 & -10 \\ -10 & 5\end{bmatrix}
                \begin{bmatrix}17 \\ 51\end{bmatrix}
                =
                \begin{bmatrix}935 - 510 \\ -170 + 255\end{bmatrix}
                =
                \begin{bmatrix}425 \\ 85\end{bmatrix}
                \]</p>

            <p class="math-display">\[
                \hat{\beta}_{ridge} = \frac{1}{55}\begin{bmatrix}425 \\ 85\end{bmatrix}
                = \begin{bmatrix}0.309 \\ 1.545\end{bmatrix}
                \]</p>

            <p>
                Final model:
            </p>

            <p class="math-display">\[
                \hat{y} = 0.309 + 1.545x
                \]</p>

            <hr>

            <h2>4. Manual Calculation in Python (NumPy)</h2>
            <pre><code class="language-python">
import numpy as np

# Design matrix with intercept
X = np.array([[1,1],[1,2],[1,3],[1,4]])
y = np.array([2,3,5,7])
lmbda = 1

XtX = X.T @ X
Xty = X.T @ y
ridge_matrix = XtX + lmbda * np.eye(XtX.shape[0])
ridge_inv = np.linalg.inv(ridge_matrix)
beta_ridge = ridge_inv @ Xty

print("Ridge coefficients:", beta_ridge)
</code></pre>

            <hr>

            <h2>5. Ridge in Scikit-learn</h2>
            <pre><code class="language-python">
from sklearn.linear_model import Ridge
import numpy as np

X_feature = np.array([[1],[2],[3],[4]])
y = np.array([2,3,5,7])

ridge = Ridge(alpha=1, fit_intercept=True)
ridge.fit(X_feature, y)

print("Intercept:", ridge.intercept_)
print("Coefficient:", ridge.coef_)
</code></pre>

            <hr>

            <h2>6. Visualizing Ridge Effect</h2>

            <h3>(a) Coefficients vs Œª</h3>
            <img class="img-fluid" src="/img/posts/ridge-post/ridge_coeffs.png" alt="Ridge coefficients vs lambda">
            <p>As Œª increases, coefficients shrink toward zero. This prevents unstable, extreme values.</p>

            <h3>(b) Error vs Œª</h3>
            <img class="img-fluid" src="/img/posts/ridge-post/ridge_error.png" alt="Ridge error vs lambda">
            <p>
                For very small Œª, the model can overfit (low bias, high variance).
                For very large Œª, it underfits (high bias, low variance).
                <strong>The sweet spot is at an intermediate Œª, which is why we usually pick it using
                    cross-validation.</strong>
            </p>

            <hr>

            <h2>7. When Should You Use Ridge?</h2>
            <ul>
                <li>When features are correlated (multicollinearity).</li>
                <li>When you care more about prediction than coefficient interpretation.</li>
                <li>When you want to reduce variance at the cost of some bias.</li>
            </ul>

            <hr>

            <h2>8. Pros and Cons</h2>
            <h3>Pros</h3>
            <ul>
                <li>Stabilizes regression with correlated predictors.</li>
                <li>Improves prediction accuracy on unseen data.</li>
                <li>Always solvable (matrix invertibility guaranteed).</li>
            </ul>
            <h3>Cons</h3>
            <ul>
                <li>Does not perform feature selection (coefficients never become exactly zero).</li>
                <li>Choice of Œª is critical and must be tuned (cross-validation).</li>
                <li>Coefficients are harder to interpret after shrinkage.</li>
            </ul>

            <hr>

            <h2>9. Key Takeaways</h2>
            <ul>
                <li>Ridge adds an L2 penalty that shrinks coefficients.</li>
                <li>It‚Äôs useful for noisy, correlated datasets.</li>
                <li>Œª controls the bias-variance tradeoff.</li>
                <li>Great entry point for learning regularization.</li>
            </ul>

            <hr>

            <h2>10. Conclusion</h2>
            <p>
                Ridge Regression shows that <strong>a slightly biased model can generalize better</strong>
                than an unbiased but unstable model.
                By controlling coefficient size, Ridge helps create robust and reliable predictive models.
            </p>

            <hr>

            <h2>References</h2>
            <ul>
                <li><a href="https://en.wikipedia.org/wiki/Ridge_regression">Wikipedia: Ridge Regression</a></li>
                <li><a href="https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression">Scikit-learn
                        Documentation</a></li>
                <li><a href="https://statweb.stanford.edu/~tibs/ElemStatLearn/">Hastie, Tibshirani, Friedman ‚Äì Elements
                        of
                        Statistical Learning</a></li>
            </ul>

            <!-- ‚úÖ BLOGPOST CONTENT ENDS -->
        {% include related-article.html %}
    </div>
    <script src="/includes/blog-style.js"></script>
</body>

</html>